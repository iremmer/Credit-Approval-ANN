{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import sys module\n",
    "import sys\n",
    "\n",
    "# Use sys.executable to ensure pip installs the package for the current version of Python\n",
    "# Install the pandas library if not already installed\n",
    "!{sys.executable} -m pip install pandas\n",
    "\n",
    "# Import pandas library for data manipulation\n",
    "import pandas as pd\n",
    "\n",
    "# Import train_test_split function for splitting data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import numpy for numerical computing\n",
    "import numpy as np\n",
    "\n",
    "# Import Sequential class for building a neural network\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# Import necessary classes for defining model architecture\n",
    "from tensorflow.keras.layers import Flatten, Dense, Activation\n",
    "\n",
    "# Import random module for generating random numbers (usage unclear)\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(matrix, multiplier):\n",
    "    \"\"\"\n",
    "    Normalizes the values in each column of the input matrix to a range between 0 and the `multiplier` argument.\n",
    "\n",
    "    Parameters:\n",
    "    matrix (numpy.ndarray): Input matrix to be normalized\n",
    "    multiplier (float): Value to which the normalized values are scaled. For example, if multiplier = 1, the normalized values will range between 0 and 1.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: Normalized matrix, with each column having values in the range of 0 to `multiplier`\n",
    "    \"\"\"\n",
    "    \n",
    "    # Iterate through each column in the matrix\n",
    "    for i in range(matrix.shape[1]):\n",
    "        # Find the maximum and minimum values in the current column\n",
    "        column_max = np.max(matrix[:, i])\n",
    "        column_min = np.min(matrix[:, i])\n",
    "        \n",
    "        # Compute the range of the current column, taking care to handle the case where max = min\n",
    "        vector_range = 1 if column_max == column_min else column_max - column_min\n",
    "        \n",
    "        # Normalize the current column to a range between 0 and `multiplier`\n",
    "        matrix[:, i] = (matrix[:, i] - column_min) / (vector_range) * multiplier\n",
    "        \n",
    "    # Return the normalized matrix\n",
    "    return matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dictionary called categorical_set with keys 'A1' through 'A15' and empty lists as values\n",
    "categorical_set = {'A1': [], 'A2': [], 'A3': [], 'A4': [], 'A5': [], 'A6': [], 'A7': [], 'A8': [], 'A9': [], 'A10': [], 'A11': [], 'A12': [], 'A13': [], 'A14': [], 'A15': []}\n",
    "\n",
    "# Define a set called continuous containing column names of the continuous variables in the dataset\n",
    "continuous = {'A2', 'A3', 'A8', 'A11', 'A14', 'A15'}\n",
    "\n",
    "# Define a list called categoricals containing column names of the categorical variables in the dataset\n",
    "categoricals = ['A1', 'A4', 'A5', 'A6', 'A7', 'A9', 'A10', 'A12', 'A13']\n",
    "\n",
    "# Open the file 'crx.data' in read mode and read all the lines into a list called lines\n",
    "with open('crx.data', 'r') as data_file:\n",
    "    lines = data_file.readlines()\n",
    "\n",
    "# Close the file 'crx.data'\n",
    "data_file.close()\n",
    "\n",
    "# Initialize an empty list called labels and a list called attributes containing the keys of categorical_set\n",
    "labels = []\n",
    "attributes = list(categorical_set.keys())\n",
    "\n",
    "# Initialize a list called previous_line with values that will be used as replacements for missing values ('?')\n",
    "previous_line = ['a', '64.08', '0.165', 'u', 'g', 'ff', 'ff', '0', 't', 't', '01', 'f', 'g', '00232', '100']\n",
    "\n",
    "# Iterate through each line in the list 'lines'\n",
    "for line in lines:\n",
    "    # Split the line into a list called 'record'\n",
    "    record = line[:-1].split(',')\n",
    "\n",
    "    # Initialize a counter to keep track of the number of missing values in the current record\n",
    "    no_question_marks = 0\n",
    "    \n",
    "    # Iterate through each attribute in 'attributes'\n",
    "    for i in range(len(attributes)):\n",
    "        # Get the key and value for the current attribute\n",
    "        key = attributes[i]\n",
    "        value = record[i]\n",
    "\n",
    "        # If the value is missing ('?'), replace it with the corresponding value from the previous line and increment the counter\n",
    "        if value == '?':\n",
    "            value = previous_line[i]\n",
    "            no_question_marks += 1\n",
    "\n",
    "        # Convert the value to a float if it corresponds to a continuous variable, otherwise leave it as a string\n",
    "        value = -np.float32(value) if key in continuous else value\n",
    "\n",
    "        # Append the value to the corresponding list in 'categorical_set'\n",
    "        categorical_set[key].append(value)\n",
    "\n",
    "    # Append 1 to 'labels' if the last element in the current record is '+', otherwise append 0\n",
    "    labels.append(1) if record[-1] == '+' else labels.append(0)\n",
    "    \n",
    "    # If there were missing values in the current record, replace 'previous_line' with the current record\n",
    "    # so that missing values in the next record can be replaced with the corresponding values from the current record\n",
    "    previous_line = record if no_question_marks == 0 else previous_line\n",
    "\n",
    "# Convert the dictionary 'categorical_set' into a pandas DataFrame called 'data'\n",
    "data = pd.DataFrame(categorical_set)\n",
    "\n",
    "# Encode the categorical variables in 'data' using one-hot encoding and store the result in a new DataFrame called 'encoded_data'\n",
    "encoded_data = pd.get_dummies(data, categoricals)\n",
    "\n",
    "# Get the column names of the encoded variables\n",
    "encoded_columns = encoded_data.columns\n",
    "\n",
    "# Normalize the values in 'encoded_data' to a range between 0 and 1 and store the result in a numpy array\n",
    "normalized_data = normalize(encoded_data.to_numpy(), 1)\n",
    "\n",
    "# Convert the normalized numpy array to a pandas DataFrame called 'encoded_data'\n",
    "encoded_data = pd.DataFrame(normalized_data)\n",
    "\n",
    "# Set the column names of 'encoded_data' to be the same as the column names of the original encoded variables\n",
    "encoded_data.columns = encoded_columns\n",
    "\n",
    "# Convert the list 'labels' into a pandas DataFrame called 'labels'\n",
    "labels = pd.DataFrame(labels)\n",
    "\n",
    "# Write the normalized encoded variables to a CSV file called 'samples.csv' without including the index column\n",
    "encoded_data.to_csv('samples.csv', index=False)\n",
    "\n",
    "# Write the labels to a CSV file called 'labels.csv' without including the index column\n",
    "labels.to_csv('labels.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file 'samples.csv' into a pandas DataFrame called 'encoded_data'\n",
    "encoded_data = pd.read_csv('samples.csv')\n",
    "\n",
    "# Read the CSV file 'labels.csv' into a pandas DataFrame called 'labels'\n",
    "labels = pd.read_csv('labels.csv')\n",
    "\n",
    "# Set the seed for the random number generator to ensure reproducibility\n",
    "random.seed(150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the encoded variables and labels into training and testing sets using the 'train_test_split' function from scikit-learn\n",
    "# The 'encoded_data' DataFrame contains the encoded variables, and the 'labels' DataFrame contains the labels\n",
    "# The training set will contain 80% of the data, and the testing set will contain 20% of the data\n",
    "# The random state is set to 2164 to ensure reproducibility of the results\n",
    "x_train, x_test, y_train, y_test = train_test_split(encoded_data, labels, test_size=0.2, random_state=2164)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "10/10 [==============================] - 1s 7ms/step - loss: 0.4883 - accuracy: 0.7953\n",
      "Epoch 2/7\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.3357 - accuracy: 0.8786\n",
      "Epoch 3/7\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.3251 - accuracy: 0.8678\n",
      "Epoch 4/7\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 0.2873 - accuracy: 0.8967\n",
      "Epoch 5/7\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.3035 - accuracy: 0.8895\n",
      "Epoch 6/7\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.2713 - accuracy: 0.8986\n",
      "Epoch 7/7\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.2622 - accuracy: 0.8986\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x21c12fb1ae0>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a sequential model with 5 dense layers\n",
    "# The first 4 layers each have 1024, 256, 128, and 32 neurons, respectively, and use ReLU activation\n",
    "# The final layer has a single neuron and uses sigmoid activation\n",
    "model = Sequential([\n",
    "    Dense(1024, activation='relu'),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model with Adam optimizer, binary crossentropy loss, and accuracy as the evaluation metric\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model using the training set with 7 epochs and a batch size of 60\n",
    "model.fit(x_train, y_train, epochs=7, batch_size=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss, test acc: [0.42948025465011597, 0.8550724387168884]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the trained model on the testing set using the 'evaluate' method\n",
    "# The 'evaluate' method returns the loss and accuracy of the model on the testing set\n",
    "results = model.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "# Print the loss and accuracy of the model on the testing set\n",
    "print('test loss, test acc:', results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
